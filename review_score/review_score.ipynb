{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mZzuwVtDYvvD",
        "outputId": "8455e3c1-0ee2-4095-86fb-dc193381a297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: fsspec, datasets\n",
            "\u001b[2K  Attempting uninstall: fsspec\n",
            "\u001b[2K    Found existing installation: fsspec 2025.3.2\n",
            "\u001b[2K    Uninstalling fsspec-2025.3.2:\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[2K  Attempting uninstall: datasets\n",
            "\u001b[2K    Found existing installation: datasets 2.14.4\n",
            "\u001b[2K    Uninstalling datasets-2.14.4:\n",
            "\u001b[2K      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [datasets]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets"
                ]
              },
              "id": "3f53c2d97b764ab59db0cf4275377d26"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 설치\n",
        "!pip install transformers konlpy scikit-learn pandas tqdm --quiet\n",
        "\n",
        "# 2. 파일 불러오기\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "goodoc = pd.read_csv('goodoc_reviews.csv')\n",
        "modoodoc = pd.read_csv('modoodoc_reviews.csv')\n",
        "\n",
        "# 3. 리뷰 데이터 통합 (긍정/부정 컬럼 상관없이 모두)\n",
        "all_data = []\n",
        "def add_review_rows(df, hospital_col, pos_col, neg_col):\n",
        "    for _, row in df.iterrows():\n",
        "        hosp = row[hospital_col]\n",
        "        for col in [pos_col, neg_col]:\n",
        "            if pd.notnull(row.get(col, None)):\n",
        "                for r in str(row[col]).split('\\n'):\n",
        "                    if r.strip():\n",
        "                        all_data.append({'hospital': hosp, 'review': r.strip()})\n",
        "\n",
        "add_review_rows(goodoc, 'hospital_name', 'positive', 'negative')\n",
        "add_review_rows(modoodoc, '병원명', '긍정리뷰', '부정리뷰')\n",
        "df = pd.DataFrame(all_data).drop_duplicates(subset=['hospital', 'review'])\n",
        "\n",
        "# 4. 모델 불러와서 자동 감성 분류(긍정:1, 부정:0)\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"beomi/KcELECTRA-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        pred = logits.argmax(-1).item()\n",
        "    return pred\n",
        "\n",
        "df['label'] = df['review'].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5. 의미 키워드(불용어/동의어) 세팅\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "STOPWORDS = set(['병원','의사','진료','환자','간호사','선생님','수술','약','내원','진단','치료','처방','이용','상담','원장','방문','인증','영수증','센터','한의원','의원','치과'])\n",
        "\n",
        "MEANING_KEYWORDS = {\n",
        "    \"친절\": [\"친절\", \"친근\", \"상냥\", \"잘해주\", \"배려\", \"따뜻\"],\n",
        "    \"불친절\": [\"불친절\", \"불쾌\", \"차가움\", \"쌀쌀\"],\n",
        "    \"자세한 설명\": [\"설명\", \"상세\", \"이해시\", \"알려\", \"자세히\"],\n",
        "    \"대기시간\": [\"대기\", \"대기시간\", \"기다림\", \"기다렸\", \"줄서\", \"오래\", \"한참\"],\n",
        "    \"과잉진료\": [\"과잉\", \"과잉진료\", \"불필요\", \"쓸데없\", \"과하게\", \"돈만\"],\n",
        "    \"비쌈\": [\"비쌈\", \"비싸\", \"가격\", \"진료비\", \"돈\", \"비용\"],\n",
        "    \"저렴\": [\"저렴\", \"싸\"],\n",
        "    \"청결\": [\"청결\", \"깨끗\", \"위생\"],\n",
        "    \"시설\": [\"시설\", \"인테리어\", \"환경\"],\n",
        "    \"추천\": [\"추천\", \"추천함\", \"강추\"],\n",
        "    \"신속\": [\"신속\", \"빠르\", \"빨랐\"],\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return ' '.join([w for w in text.split() if w not in STOPWORDS])\n",
        "\n",
        "def meaning_tokens(text):\n",
        "    hits = []\n",
        "    for mean, arr in MEANING_KEYWORDS.items():\n",
        "        for kw in arr:\n",
        "            if kw in text:\n",
        "                hits.append(mean)\n",
        "    return hits\n",
        "\n",
        "df['review_clean'] = df['review'].apply(clean_text)\n",
        "df['meaning_kw'] = df['review_clean'].apply(meaning_tokens)\n",
        "\n",
        "# 6. 병원별 top3 키워드, 점수화(긍정: +1, 부정: -1, 인증/방문/영수증 언급시 가중치 1.5배)\n",
        "result = []\n",
        "for hosp, g in df.groupby('hospital'):\n",
        "    tokens = []\n",
        "    score = 0\n",
        "    for _, row in g.iterrows():\n",
        "        tokens += row['meaning_kw']\n",
        "        # 가중치\n",
        "        w = 1.0\n",
        "        if any(word in row['review'] for word in ['영수증', '인증', '방문']):\n",
        "            w = 1.5\n",
        "        if row['label'] == 1:\n",
        "            score += w\n",
        "        else:\n",
        "            score -= w\n",
        "    top3 = [w for w, _ in Counter(tokens).most_common(3)]\n",
        "    result.append({\n",
        "        'hospital': hosp,\n",
        "        'score': score,\n",
        "        'top_keywords': ','.join(top3),\n",
        "        'n_review': len(g)\n",
        "    })\n",
        "\n",
        "df_rank = pd.DataFrame(result).sort_values('score', ascending=False)\n",
        "print(df_rank.head(10))\n",
        "df_rank.to_csv('병원별_자동감성_top3키워드_점수화.csv', index=False)"
      ],
      "metadata": {
        "id": "p1epwJnjQWRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf22abd-7530-4982-cedf-0aac571c9bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 3945/3945 [12:33<00:00,  5.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          hospital  score    top_keywords  n_review\n",
            "380       사계절에스한의원   19.5      친절,추천,대기시간        38\n",
            "1122      힐링산부인과의원   19.0  친절,자세한 설명,대기시간        53\n",
            "151        노원 명한의원   15.5    친절,자세한 설명,추천        46\n",
            "649         여진주한의원   13.5    친절,추천,자세한 설명        44\n",
            "235    디딤정신건강의학과의원   13.5  친절,자세한 설명,과잉진료        22\n",
            "859      인애한의원 노원점   13.0    친절,추천,자세한 설명        33\n",
            "782   유앤영피부과의원 노원점   12.0      대기시간,친절,추천        71\n",
            "1070        한의원혜민서   11.0    친절,추천,자세한 설명        35\n",
            "403     상계바론정형외과의원   11.0  자세한 설명,비쌈,대기시간        30\n",
            "740      올리브산부인과의원    8.0  자세한 설명,친절,과잉진료        62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# 1. 의미 키워드와 불용어(아까 준 것 그대로)\n",
        "STOPWORDS = set([\n",
        "    '병원','의사','진료','환자','간호사','선생님','수술','약','내원','진단','치료',\n",
        "    '처방','이용','상담','원장','방문','인증','영수증','센터','한의원','의원','치과'\n",
        "])\n",
        "\n",
        "MEANING_KEYWORDS = {\n",
        "    \"친절\": [\"친절\", \"친근\", \"상냥\", \"잘해주\", \"배려\", \"따뜻\"],\n",
        "    \"불친절\": [\"불친절\", \"불쾌\", \"차가움\", \"쌀쌀\"],\n",
        "    \"자세한 설명\": [\"설명\", \"상세\", \"이해시\", \"알려\", \"자세히\"],\n",
        "    \"대기시간\": [\"대기\", \"대기시간\", \"기다림\", \"기다렸\", \"줄서\", \"오래\", \"한참\"],\n",
        "    \"과잉진료\": [\"과잉\", \"과잉진료\", \"불필요\", \"쓸데없\", \"과하게\", \"돈만\"],\n",
        "    \"비쌈\": [\"비쌈\", \"비싸\", \"가격\", \"진료비\", \"돈\", \"비용\"],\n",
        "    \"저렴\": [\"저렴\", \"싸\"],\n",
        "    \"청결\": [\"청결\", \"깨끗\", \"위생\"],\n",
        "    \"시설\": [\"시설\", \"인테리어\", \"환경\"],\n",
        "    \"추천\": [\"추천\", \"추천함\", \"강추\"],\n",
        "    \"신속\": [\"신속\", \"빠르\", \"빨랐\"],\n",
        "}\n",
        "\n",
        "def meaning_tokens(tokens):\n",
        "    hits = []\n",
        "    for t in tokens:\n",
        "        if t in STOPWORDS:\n",
        "            continue\n",
        "        for mean, arr in MEANING_KEYWORDS.items():\n",
        "            if t in arr:\n",
        "                hits.append(mean)\n",
        "                break\n",
        "    return hits\n",
        "\n",
        "# 2. 파일 불러서 병원별 의미키워드 카운트\n",
        "files = ['goodoc_token.json', 'modoodoc_token.json']\n",
        "all_tokens = defaultdict(list)\n",
        "for file in files:\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        for r in json.load(f):\n",
        "            all_tokens[r['hospital_name']].extend(r['tokens'])\n",
        "\n",
        "hospital_meaning_kw = {}\n",
        "for hosp, tokens in all_tokens.items():\n",
        "    mapped = meaning_tokens(tokens)\n",
        "    hospital_meaning_kw[hosp] = Counter(mapped)\n",
        "\n",
        "# 3. 예시 — 한 병원 토큰/매핑 결과 확인\n",
        "ex_hosp = \"마들수내과의원\"\n",
        "print('리뷰 토큰:', all_tokens[ex_hosp][:30])     # 30개만 샘플\n",
        "print('의미키워드 카운트:', hospital_meaning_kw[ex_hosp])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR7HvvHqRO68",
        "outputId": "2ca50816-119b-4b70-f156-f16f4675be9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰 토큰: ['어느', '##때', '##나가', '##도', '대기', '##환', '##자', '##가', '여럿', '##있', '##지만', '직원', '##분', '##들이', '많', '##아', '##서', '오랫동안', '##은', '안기', '##다', '##렸', '##고', ',', '서류', '발급', '##도', '바로', '##바로', '##해']\n",
            "의미키워드 카운트: Counter({'친절': 39, '자세한 설명': 16, '대기시간': 14, '청결': 9, '추천': 9, '시설': 4, '신속': 3, '비쌈': 2, '과잉진료': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_rank = pd.read_csv('/content/병원별_자동감성_top3키워드_점수화.csv')\n",
        "\n",
        "def keyword_coverage(hospital, top_keywords):\n",
        "    top3 = [k.strip() for k in str(top_keywords).split(',') if k.strip()]\n",
        "    mapped_counter = hospital_meaning_kw.get(hospital, {})\n",
        "    matched = [k for k in top3 if mapped_counter.get(k, 0) > 0]\n",
        "    freq = {k: mapped_counter.get(k, 0) for k in top3}\n",
        "    coverage = len(matched) / len(top3) if top3 else 0\n",
        "    return coverage, freq, matched\n",
        "\n",
        "df_rank['keyword_coverage'] = df_rank.apply(\n",
        "    lambda row: keyword_coverage(row['hospital'], row['top_keywords'])[0], axis=1)\n",
        "df_rank['keyword_freq'] = df_rank.apply(\n",
        "    lambda row: keyword_coverage(row['hospital'], row['top_keywords'])[1], axis=1)\n",
        "df_rank['keyword_matched'] = df_rank.apply(\n",
        "    lambda row: keyword_coverage(row['hospital'], row['top_keywords'])[2], axis=1)\n",
        "\n",
        "print(df_rank[['hospital', 'top_keywords', 'keyword_coverage', 'keyword_freq', 'keyword_matched']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56hsLBBnSWEH",
        "outputId": "cf156ca4-d7cb-4950-8f82-f80e024e15a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       hospital    top_keywords  keyword_coverage  \\\n",
            "0      사계절에스한의원      친절,추천,대기시간               1.0   \n",
            "1      힐링산부인과의원  친절,자세한 설명,대기시간               1.0   \n",
            "2       노원 명한의원    친절,자세한 설명,추천               1.0   \n",
            "3        여진주한의원    친절,추천,자세한 설명               1.0   \n",
            "4   디딤정신건강의학과의원  친절,자세한 설명,과잉진료               1.0   \n",
            "5     인애한의원 노원점    친절,추천,자세한 설명               1.0   \n",
            "6  유앤영피부과의원 노원점      대기시간,친절,추천               1.0   \n",
            "7        한의원혜민서    친절,추천,자세한 설명               1.0   \n",
            "8    상계바론정형외과의원  자세한 설명,비쌈,대기시간               1.0   \n",
            "9     올리브산부인과의원  자세한 설명,친절,과잉진료               1.0   \n",
            "\n",
            "                           keyword_freq     keyword_matched  \n",
            "0        {'친절': 17, '추천': 4, '대기시간': 1}      [친절, 추천, 대기시간]  \n",
            "1   {'친절': 23, '자세한 설명': 24, '대기시간': 7}  [친절, 자세한 설명, 대기시간]  \n",
            "2     {'친절': 20, '자세한 설명': 11, '추천': 7}    [친절, 자세한 설명, 추천]  \n",
            "3    {'친절': 24, '추천': 13, '자세한 설명': 13}    [친절, 추천, 자세한 설명]  \n",
            "4    {'친절': 15, '자세한 설명': 2, '과잉진료': 2}  [친절, 자세한 설명, 과잉진료]  \n",
            "5       {'친절': 9, '추천': 3, '자세한 설명': 1}    [친절, 추천, 자세한 설명]  \n",
            "6      {'대기시간': 11, '친절': 24, '추천': 15}      [대기시간, 친절, 추천]  \n",
            "7      {'친절': 21, '추천': 6, '자세한 설명': 4}    [친절, 추천, 자세한 설명]  \n",
            "8    {'자세한 설명': 9, '비쌈': 6, '대기시간': 10}  [자세한 설명, 비쌈, 대기시간]  \n",
            "9  {'자세한 설명': 39, '친절': 54, '과잉진료': 14}  [자세한 설명, 친절, 과잉진료]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rank.to_csv(\"병원별_키워드_분석.csv\", index=False)"
      ],
      "metadata": {
        "id": "uUPlnBsPS0GD"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}